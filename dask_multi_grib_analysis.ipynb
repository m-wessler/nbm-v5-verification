{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pygrib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Dask version: {dask.__version__}\")\n",
    "print(f\"XArray version: {xr.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a1cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Dask cluster for parallel processing\n",
    "# Adjust memory limits based on your system (each GRIB ~480MB)\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,  # Adjust based on CPU cores\n",
    "    threads_per_worker=2,\n",
    "    memory_limit='2GB',  # Per worker memory limit\n",
    "    dashboard_address=':8787'  # View dashboard at localhost:8787\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n",
    "print(f\"Dask Dashboard: {client.dashboard_link}\")\n",
    "print(f\"Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration based on our proof-of-concept\n",
    "data_dir = Path('./data')\n",
    "selected_variable = 'jfwprb'\n",
    "selected_init_hour = '00'\n",
    "selected_date = '20251001'  # Focus on one date initially\n",
    "\n",
    "# Variable names from our proof-of-concept\n",
    "variable_names = [\n",
    "    \"H35S10\", \"H30S15\", \"H25S10\", \"H25S15\", \"H25S20\", \n",
    "    \"H20S15\", \"H20S20\", \"H20S30\", \"H15S15\", \"H15S20\", \n",
    "    \"H10S30\", \"H35G25\", \"H25G30\", \"H25G55\", \"H15G25\", \"H15G35\"\n",
    "]\n",
    "\n",
    "print(f\"Target directory: {data_dir / selected_date / selected_init_hour}\")\n",
    "print(f\"Variables per file: {len(variable_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover all JFWPRB files for the selected date/init hour\n",
    "target_dir = data_dir / selected_date / selected_init_hour\n",
    "\n",
    "if target_dir.exists():\n",
    "    # Find all JFWPRB files and sort by forecast hour\n",
    "    jfwprb_files = sorted([\n",
    "        f for f in target_dir.iterdir() \n",
    "        if f.is_file() and f.name.startswith(selected_variable)\n",
    "    ])\n",
    "    \n",
    "    print(f\"Found {len(jfwprb_files)} JFWPRB files:\")\n",
    "    \n",
    "    # Extract forecast hours and file sizes\n",
    "    file_info = []\n",
    "    total_size = 0\n",
    "    \n",
    "    for file_path in jfwprb_files:\n",
    "        # Extract forecast hour from filename (e.g., jfwprb_qmd_f003.grib2 -> 3)\n",
    "        fname = file_path.stem\n",
    "        try:\n",
    "            fhour = int(fname.split('_f')[-1])\n",
    "        except:\n",
    "            fhour = 0\n",
    "        \n",
    "        size_mb = file_path.stat().st_size / (1024**2)\n",
    "        total_size += size_mb\n",
    "        \n",
    "        file_info.append({\n",
    "            'file': file_path.name,\n",
    "            'forecast_hour': fhour,\n",
    "            'size_mb': size_mb,\n",
    "            'path': file_path\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame for easy analysis\n",
    "    files_df = pd.DataFrame(file_info).sort_values('forecast_hour')\n",
    "    \n",
    "    print(f\"\\nForecast hours: {files_df['forecast_hour'].min()} to {files_df['forecast_hour'].max()}\")\n",
    "    print(f\"File sizes: {files_df['size_mb'].min():.1f} - {files_df['size_mb'].max():.1f} MB\")\n",
    "    print(f\"Total data: {total_size:.1f} MB ({total_size/1024:.2f} GB)\")\n",
    "    \n",
    "    # Show first few files\n",
    "    print(f\"\\nFirst 10 files:\")\n",
    "    print(files_df[['file', 'forecast_hour', 'size_mb']].head(10).to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Directory not found: {target_dir}\")\n",
    "    files_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract metadata from a single GRIB file (optimized for Dask)\n",
    "@dask.delayed\n",
    "def extract_grib_metadata(file_path):\n",
    "    \"\"\"\n",
    "    Extract essential metadata from GRIB file without loading full data.\n",
    "    Returns projection info, grid dimensions, and forecast metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        grbs = pygrib.open(str(file_path))\n",
    "        first_grb = grbs[1]\n",
    "        \n",
    "        # Extract key metadata\n",
    "        metadata = {\n",
    "            'file_path': str(file_path),\n",
    "            'n_messages': grbs.messages,\n",
    "            'forecast_hour': int(file_path.stem.split('_f')[-1]) if '_f' in file_path.stem else 0,\n",
    "        }\n",
    "        \n",
    "        # Grid information\n",
    "        try:\n",
    "            metadata.update({\n",
    "                'grid_type': first_grb['gridType'],\n",
    "                'Ni': first_grb['Ni'],\n",
    "                'Nj': first_grb['Nj'],\n",
    "                'data_date': first_grb['dataDate'],\n",
    "                'data_time': first_grb['dataTime'],\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # Projection parameters (Lambert Conformal)\n",
    "        try:\n",
    "            metadata.update({\n",
    "                'projection': 'lambert_conformal_conic',\n",
    "                'standard_parallel_1': first_grb['Latin1InDegrees'],\n",
    "                'central_longitude': first_grb['LoVInDegrees'],\n",
    "                'central_latitude': first_grb['LaDInDegrees'],\n",
    "            })\n",
    "        except:\n",
    "            metadata['projection'] = 'latitude_longitude'\n",
    "        \n",
    "        grbs.close()\n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'file_path': str(file_path), 'error': str(e)}\n",
    "\n",
    "# Extract metadata from all files in parallel\n",
    "if not files_df.empty:\n",
    "    print(\"üîç Extracting metadata from all files in parallel...\")\n",
    "    \n",
    "    # Create delayed tasks for each file\n",
    "    metadata_tasks = [extract_grib_metadata(row['path']) for _, row in files_df.iterrows()]\n",
    "    \n",
    "    # Execute in parallel\n",
    "    metadata_results = dask.compute(*metadata_tasks)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    metadata_df = pd.DataFrame(metadata_results)\n",
    "    \n",
    "    print(f\"‚úÖ Processed {len(metadata_df)} files\")\n",
    "    print(f\"\\nMetadata summary:\")\n",
    "    print(metadata_df[['forecast_hour', 'n_messages', 'grid_type', 'projection']].head(10))\n",
    "else:\n",
    "    metadata_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b47be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a single GRIB file as xarray Dataset (Dask-compatible)\n",
    "@dask.delayed\n",
    "def load_grib_to_xarray(file_path, variable_names, chunk_size='100MB'):\n",
    "    \"\"\"\n",
    "    Load GRIB file into xarray Dataset with Dask arrays for memory efficiency.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        grbs = pygrib.open(str(file_path))\n",
    "        \n",
    "        # Extract forecast hour from filename\n",
    "        forecast_hour = int(file_path.stem.split('_f')[-1]) if '_f' in file_path.stem else 0\n",
    "        \n",
    "        # Read messages and projection info\n",
    "        messages_data = []\n",
    "        lats, lons = None, None\n",
    "        projection_info = {}\n",
    "        \n",
    "        for i, grb in enumerate(grbs):\n",
    "            if i >= len(variable_names):  # Only take first 16 messages\n",
    "                break\n",
    "                \n",
    "            data = grb.values\n",
    "            messages_data.append(data)\n",
    "            \n",
    "            # Get coordinates from first message\n",
    "            if lats is None:\n",
    "                lats, lons = grb.latlons()\n",
    "                \n",
    "                # Extract projection info\n",
    "                try:\n",
    "                    projection_info = {\n",
    "                        'grid_type': grb['gridType'],\n",
    "                        'projection': 'lambert_conformal_conic',\n",
    "                        'standard_parallel_1': grb['Latin1InDegrees'],\n",
    "                        'central_longitude': grb['LoVInDegrees'],\n",
    "                        'central_latitude': grb['LaDInDegrees'],\n",
    "                    }\n",
    "                except:\n",
    "                    projection_info = {'projection': 'latitude_longitude'}\n",
    "        \n",
    "        grbs.close()\n",
    "        \n",
    "        if not messages_data:\n",
    "            return None\n",
    "            \n",
    "        # Create coordinate arrays\n",
    "        lat_1d = lats[:, 0]\n",
    "        lon_1d = lons[0, :]\n",
    "        \n",
    "        # Convert to Dask arrays for memory efficiency\n",
    "        data_stack = np.stack(messages_data[:len(variable_names)], axis=0)\n",
    "        \n",
    "        # Create Dask array with appropriate chunking\n",
    "        dask_data = da.from_array(data_stack, chunks=(1, 'auto', 'auto'))\n",
    "        \n",
    "        # Create coordinates\n",
    "        coords = {\n",
    "            'variable': variable_names[:len(messages_data)],\n",
    "            'latitude': ('latitude', lat_1d, {\n",
    "                'units': 'degrees_north', \n",
    "                'standard_name': 'latitude'\n",
    "            }),\n",
    "            'longitude': ('longitude', lon_1d, {\n",
    "                'units': 'degrees_east', \n",
    "                'standard_name': 'longitude'\n",
    "            }),\n",
    "            'forecast_hour': forecast_hour\n",
    "        }\n",
    "        \n",
    "        # Create Dataset with Dask arrays\n",
    "        ds = xr.Dataset({\n",
    "            'fire_weather_prob': (\n",
    "                ['variable', 'latitude', 'longitude'], \n",
    "                dask_data,\n",
    "                {\n",
    "                    'long_name': 'Fire Weather Probability',\n",
    "                    'units': 'dimensionless',\n",
    "                    'grid_mapping': 'crs'\n",
    "                }\n",
    "            )\n",
    "        }, coords=coords)\n",
    "        \n",
    "        # Add global attributes\n",
    "        ds.attrs = {\n",
    "            'title': f'JFWPRB Fire Weather Probability - Hour {forecast_hour:03d}',\n",
    "            'source_file': file_path.name,\n",
    "            'forecast_hour': forecast_hour,\n",
    "            'Conventions': 'CF-1.8',\n",
    "            **{f'grid_{k}': v for k, v in projection_info.items()}\n",
    "        }\n",
    "        \n",
    "        # Add CRS coordinate\n",
    "        if projection_info.get('projection') == 'lambert_conformal_conic':\n",
    "            crs_attrs = {\n",
    "                'grid_mapping_name': 'lambert_conformal_conic',\n",
    "                'standard_parallel': [projection_info.get('standard_parallel_1', 25.0)],\n",
    "                'longitude_of_central_meridian': projection_info.get('central_longitude', -95.0),\n",
    "                'latitude_of_projection_origin': projection_info.get('central_latitude', 25.0),\n",
    "            }\n",
    "        else:\n",
    "            crs_attrs = {'grid_mapping_name': 'latitude_longitude'}\n",
    "            \n",
    "        ds = ds.assign_coords(crs=xr.DataArray(0, attrs=crs_attrs))\n",
    "        \n",
    "        return ds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Dask-compatible GRIB loader function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37722e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a subset of files for testing (start with first 5 forecast hours)\n",
    "if not files_df.empty:\n",
    "    # Select first 5 files for proof-of-concept\n",
    "    test_files = files_df.head(5)['path'].tolist()\n",
    "    \n",
    "    print(f\"üöÄ Loading {len(test_files)} files with Dask...\")\n",
    "    print(f\"Files: {[f.name for f in test_files]}\")\n",
    "    \n",
    "    # Create delayed tasks for parallel loading\n",
    "    load_tasks = [load_grib_to_xarray(file_path, variable_names) for file_path in test_files]\n",
    "    \n",
    "    # Execute loading in parallel\n",
    "    with dask.config.set(scheduler='threads'):  # Use threads for I/O bound tasks\n",
    "        datasets = dask.compute(*load_tasks)\n",
    "    \n",
    "    # Filter out None results\n",
    "    valid_datasets = [ds for ds in datasets if ds is not None]\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded {len(valid_datasets)} datasets\")\n",
    "    \n",
    "    if valid_datasets:\n",
    "        # Show info about first dataset\n",
    "        first_ds = valid_datasets[0]\n",
    "        print(f\"\\nFirst dataset info:\")\n",
    "        print(f\"  Shape: {first_ds.fire_weather_prob.shape}\")\n",
    "        print(f\"  Chunks: {first_ds.fire_weather_prob.chunks}\")\n",
    "        print(f\"  Memory usage: ~{first_ds.fire_weather_prob.nbytes / 1024**2:.1f} MB\")\n",
    "        print(f\"  Variables: {list(first_ds.coords['variable'].values)}\")\n",
    "        print(f\"  Forecast hour: {first_ds.attrs['forecast_hour']}\")\n",
    "        \n",
    "else:\n",
    "    valid_datasets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets along time dimension using Dask\n",
    "if valid_datasets:\n",
    "    print(\"üîÑ Combining datasets along forecast time dimension...\")\n",
    "    \n",
    "    # Sort datasets by forecast hour\n",
    "    sorted_datasets = sorted(valid_datasets, key=lambda ds: ds.attrs['forecast_hour'])\n",
    "    \n",
    "    # Create forecast_hour coordinate for each dataset\n",
    "    for ds in sorted_datasets:\n",
    "        fhour = ds.attrs['forecast_hour']\n",
    "        ds.coords['forecast_hour'] = fhour\n",
    "    \n",
    "    # Concatenate along new 'time' dimension\n",
    "    try:\n",
    "        combined_ds = xr.concat(sorted_datasets, dim='forecast_hour')\n",
    "        \n",
    "        print(f\"‚úÖ Combined dataset created!\")\n",
    "        print(f\"  Final shape: {combined_ds.fire_weather_prob.shape}\")\n",
    "        print(f\"  Dimensions: {dict(combined_ds.dims)}\")\n",
    "        print(f\"  Forecast hours: {combined_ds.forecast_hour.values}\")\n",
    "        print(f\"  Total memory (if loaded): ~{combined_ds.fire_weather_prob.nbytes / 1024**3:.2f} GB\")\n",
    "        print(f\"  Chunked memory usage: ~{combined_ds.fire_weather_prob.chunks}\")\n",
    "        \n",
    "        # Update global attributes\n",
    "        combined_ds.attrs.update({\n",
    "            'title': 'Multi-Hour JFWPRB Fire Weather Probability Analysis',\n",
    "            'n_forecast_hours': len(sorted_datasets),\n",
    "            'forecast_hours': list(combined_ds.forecast_hour.values),\n",
    "            'processing': 'Dask-enabled parallel processing'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error combining datasets: {e}\")\n",
    "        combined_ds = None\n",
    "else:\n",
    "    combined_ds = None\n",
    "    print(\"‚ùå No valid datasets to combine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Dask-powered analysis operations\n",
    "if combined_ds is not None:\n",
    "    print(\"üìä DASK-POWERED ANALYSIS OPERATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Lazy computation examples (no data loaded yet)\n",
    "    print(\"\\nüöÄ Setting up lazy computations...\")\n",
    "    \n",
    "    # Time-maximum across forecast hours for each variable\n",
    "    max_over_time = combined_ds.fire_weather_prob.max(dim='forecast_hour')\n",
    "    print(f\"  Time maximum: {max_over_time.shape} - {type(max_over_time.data)}\")\n",
    "    \n",
    "    # Mean across all variables for each forecast hour\n",
    "    mean_by_hour = combined_ds.fire_weather_prob.mean(dim='variable')\n",
    "    print(f\"  Variable mean: {mean_by_hour.shape} - {type(mean_by_hour.data)}\")\n",
    "    \n",
    "    # Standard deviation across forecast hours\n",
    "    std_over_time = combined_ds.fire_weather_prob.std(dim='forecast_hour')\n",
    "    print(f\"  Time std dev: {std_over_time.shape} - {type(std_over_time.data)}\")\n",
    "    \n",
    "    # 2. Execute a subset of computations\n",
    "    print(\"\\n‚ö° Computing maximum values across time...\")\n",
    "    \n",
    "    # Select specific variables for faster computation\n",
    "    key_vars = ['H35S10', 'H25G55', 'H15G25']  # Variables that showed activity\n",
    "    subset_ds = combined_ds.sel(variable=key_vars)\n",
    "    \n",
    "    # Compute maximum values (this triggers actual computation)\n",
    "    with dask.config.set(scheduler='threads'):\n",
    "        max_values = subset_ds.fire_weather_prob.max(dim=['forecast_hour', 'variable']).compute()\n",
    "    \n",
    "    print(f\"  ‚úÖ Computed! Global maximum: {max_values.values:.6f}\")\n",
    "    \n",
    "    # 3. Spatial statistics by forecast hour\n",
    "    print(\"\\nüìà Computing spatial statistics...\")\n",
    "    \n",
    "    spatial_stats = []\n",
    "    for fhour in combined_ds.forecast_hour.values[:3]:  # First 3 hours\n",
    "        hour_data = combined_ds.sel(forecast_hour=fhour).fire_weather_prob\n",
    "        \n",
    "        # Compute statistics (lazy)\n",
    "        stats = {\n",
    "            'forecast_hour': int(fhour),\n",
    "            'max_val': hour_data.max(),\n",
    "            'mean_val': hour_data.mean(),\n",
    "            'std_val': hour_data.std(),\n",
    "            'nonzero_count': (hour_data > 0).sum()\n",
    "        }\n",
    "        \n",
    "        # Compute all stats for this hour\n",
    "        with dask.config.set(scheduler='threads'):\n",
    "            computed_stats = {k: v.compute() if hasattr(v, 'compute') else v \n",
    "                            for k, v in stats.items()}\n",
    "        \n",
    "        spatial_stats.append(computed_stats)\n",
    "    \n",
    "    # Display results\n",
    "    stats_df = pd.DataFrame(spatial_stats)\n",
    "    print(f\"\\n  Spatial statistics by forecast hour:\")\n",
    "    print(stats_df.round(6))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No combined dataset available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ce4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage monitoring and optimization\n",
    "if combined_ds is not None:\n",
    "    print(\"üíæ MEMORY USAGE ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Dataset memory info\n",
    "    total_size = combined_ds.fire_weather_prob.nbytes\n",
    "    print(f\"Total dataset size: {total_size / 1024**3:.2f} GB\")\n",
    "    print(f\"Chunk sizes: {combined_ds.fire_weather_prob.chunks}\")\n",
    "    \n",
    "    # Dask task graph info\n",
    "    n_tasks = len(combined_ds.fire_weather_prob.__dask_graph__())\n",
    "    print(f\"Dask task graph size: {n_tasks} tasks\")\n",
    "    \n",
    "    # Worker memory usage\n",
    "    try:\n",
    "        worker_info = client.scheduler_info()['workers']\n",
    "        print(f\"\\nWorker memory usage:\")\n",
    "        for worker_id, info in worker_info.items():\n",
    "            memory_used = info['metrics'].get('memory', 0) / 1024**3\n",
    "            memory_limit = info.get('memory_limit', 0) / 1024**3\n",
    "            print(f\"  {worker_id.split('-')[-1]}: {memory_used:.2f} / {memory_limit:.2f} GB\")\n",
    "    except:\n",
    "        print(\"  Worker memory info not available\")\n",
    "    \n",
    "    # Optimization suggestions\n",
    "    print(f\"\\nüõ†Ô∏è  OPTIMIZATION SUGGESTIONS:\")\n",
    "    if total_size > 8 * 1024**3:  # > 8GB\n",
    "        print(\"  ‚Ä¢ Consider processing in smaller batches\")\n",
    "        print(\"  ‚Ä¢ Use .persist() to keep active data in memory\")\n",
    "        print(\"  ‚Ä¢ Increase chunk sizes for better I/O performance\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ Dataset size is manageable for most systems\")\n",
    "        print(\"  ‚Ä¢ Consider .persist() for repeated operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalable visualization with Dask\n",
    "if combined_ds is not None:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.feature as cfeature\n",
    "    \n",
    "    print(\"üé® CREATING SCALABLE VISUALIZATIONS\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # 1. Time series of maximum values (lightweight computation)\n",
    "    print(\"\\nüìà Time series analysis...\")\n",
    "    \n",
    "    # Compute max value for each forecast hour and variable\n",
    "    with dask.config.set(scheduler='threads'):\n",
    "        max_by_hour_var = combined_ds.fire_weather_prob.max(dim=['latitude', 'longitude']).compute()\n",
    "    \n",
    "    # Create time series plot\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Plot 1: Max values by forecast hour for key variables\n",
    "    key_vars = ['H35S10', 'H25G55', 'H15G25']\n",
    "    for var in key_vars:\n",
    "        if var in max_by_hour_var.variable.values:\n",
    "            data = max_by_hour_var.sel(variable=var)\n",
    "            ax1.plot(data.forecast_hour, data.values, 'o-', label=var, linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('Forecast Hour')\n",
    "    ax1.set_ylabel('Maximum Probability')\n",
    "    ax1.set_title('Fire Weather Probability - Maximum Values by Forecast Hour')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Average across all variables\n",
    "    avg_by_hour = max_by_hour_var.mean(dim='variable')\n",
    "    ax2.bar(avg_by_hour.forecast_hour, avg_by_hour.values, alpha=0.7, color='steelblue')\n",
    "    ax2.set_xlabel('Forecast Hour')\n",
    "    ax2.set_ylabel('Average Maximum Probability')\n",
    "    ax2.set_title('Average Fire Weather Probability Across All Variables')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Time series plots created\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No dataset available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ed284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Dask operations for large-scale analysis\n",
    "if combined_ds is not None:\n",
    "    print(\"üöÄ ADVANCED DASK OPERATIONS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # 1. Persist dataset in distributed memory for faster repeated access\n",
    "    print(\"\\nüíæ Persisting dataset in worker memory...\")\n",
    "    \n",
    "    # Select subset for demonstration\n",
    "    work_ds = combined_ds.isel(forecast_hour=slice(0, 3))  # First 3 hours\n",
    "    persisted_ds = work_ds.persist()\n",
    "    \n",
    "    print(f\"  ‚úÖ Dataset persisted: {persisted_ds.fire_weather_prob.shape}\")\n",
    "    \n",
    "    # 2. Parallel computation across variables\n",
    "    print(\"\\n‚ö° Computing statistics in parallel...\")\n",
    "    \n",
    "    def compute_variable_stats(ds, var_name):\n",
    "        \"\"\"Compute comprehensive statistics for a variable\"\"\"\n",
    "        var_data = ds.sel(variable=var_name).fire_weather_prob\n",
    "        \n",
    "        return {\n",
    "            'variable': var_name,\n",
    "            'global_max': var_data.max().compute().values,\n",
    "            'global_mean': var_data.mean().compute().values,\n",
    "            'temporal_std': var_data.std(dim='forecast_hour').max().compute().values,\n",
    "            'spatial_std': var_data.std(dim=['latitude', 'longitude']).max().compute().values,\n",
    "            'active_cells': (var_data > 0.01).sum().compute().values\n",
    "        }\n",
    "    \n",
    "    # Process key variables in parallel\n",
    "    key_variables = ['H35S10', 'H25S10', 'H25G55', 'H15G25']\n",
    "    \n",
    "    # Create delayed tasks\n",
    "    stat_tasks = [\n",
    "        dask.delayed(compute_variable_stats)(persisted_ds, var) \n",
    "        for var in key_variables\n",
    "    ]\n",
    "    \n",
    "    # Execute in parallel\n",
    "    variable_stats = dask.compute(*stat_tasks)\n",
    "    \n",
    "    # Display results\n",
    "    stats_df = pd.DataFrame(variable_stats)\n",
    "    print(f\"\\nüìä Variable statistics:\")\n",
    "    print(stats_df.round(6))\n",
    "    \n",
    "    # 3. Memory-efficient spatial operations\n",
    "    print(\"\\nüó∫Ô∏è  Computing spatial patterns...\")\n",
    "    \n",
    "    # Find regions of highest activity (using chunked operations)\n",
    "    max_over_time_vars = persisted_ds.fire_weather_prob.max(dim=['forecast_hour', 'variable'])\n",
    "    \n",
    "    # Use Dask to find locations above threshold\n",
    "    threshold = 0.1\n",
    "    active_mask = max_over_time_vars > threshold\n",
    "    \n",
    "    # Count active grid cells\n",
    "    n_active = active_mask.sum().compute().values\n",
    "    total_cells = active_mask.size\n",
    "    \n",
    "    print(f\"  Active cells (>{threshold}): {n_active:,} / {total_cells:,} ({100*n_active/total_cells:.2f}%)\")\n",
    "    \n",
    "    # Find maximum location\n",
    "    max_val = max_over_time_vars.max().compute().values\n",
    "    max_coords = max_over_time_vars.where(\n",
    "        max_over_time_vars == max_val, drop=True\n",
    "    ).compute()\n",
    "    \n",
    "    if max_coords.size > 0:\n",
    "        max_lat = float(max_coords.latitude.values.flatten()[0])\n",
    "        max_lon = float(max_coords.longitude.values.flatten()[0])\n",
    "        print(f\"  Maximum value: {max_val:.6f} at ({max_lat:.2f}¬∞N, {max_lon:.2f}¬∞W)\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No dataset available for advanced operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling to ALL files - production workflow\n",
    "def create_production_workflow(data_dir, date, init_hour, max_files=None):\n",
    "    \"\"\"\n",
    "    Create a production workflow for processing all JFWPRB files.\n",
    "    \"\"\"\n",
    "    print(f\"üè≠ PRODUCTION WORKFLOW SETUP\")\n",
    "    print(f\"=\" * 40)\n",
    "    \n",
    "    target_dir = data_dir / date / init_hour\n",
    "    \n",
    "    if not target_dir.exists():\n",
    "        print(f\"‚ùå Directory not found: {target_dir}\")\n",
    "        return None\n",
    "    \n",
    "    # Find all JFWPRB files\n",
    "    all_files = sorted([\n",
    "        f for f in target_dir.iterdir() \n",
    "        if f.is_file() and f.name.startswith('jfwprb')\n",
    "    ])\n",
    "    \n",
    "    if max_files:\n",
    "        all_files = all_files[:max_files]\n",
    "    \n",
    "    total_size_gb = sum(f.stat().st_size for f in all_files) / 1024**3\n",
    "    \n",
    "    print(f\"üìÅ Found {len(all_files)} JFWPRB files\")\n",
    "    print(f\"üíæ Total size: {total_size_gb:.1f} GB\")\n",
    "    print(f\"‚è±Ô∏è  Estimated processing time: {len(all_files) * 30 / 60:.1f} minutes\")\n",
    "    \n",
    "    # Batch processing strategy\n",
    "    batch_size = 10  # Process 10 files at a time\n",
    "    n_batches = (len(all_files) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\nüì¶ Batch processing strategy:\")\n",
    "    print(f\"  Batch size: {batch_size} files\")\n",
    "    print(f\"  Number of batches: {n_batches}\")\n",
    "    print(f\"  Memory per batch: ~{batch_size * 0.5:.1f} GB\")\n",
    "    \n",
    "    # Memory management recommendations\n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    if total_size_gb > 20:\n",
    "        print(f\"  ‚Ä¢ Use batch processing (implemented above)\")\n",
    "        print(f\"  ‚Ä¢ Consider increasing worker memory limits\")\n",
    "        print(f\"  ‚Ä¢ Use .persist() judiciously to avoid memory overflow\")\n",
    "        print(f\"  ‚Ä¢ Monitor Dask dashboard for memory usage\")\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ Total size is manageable for most systems\")\n",
    "        print(f\"  ‚Ä¢ Can likely process all files simultaneously\")\n",
    "    \n",
    "    return {\n",
    "        'files': all_files,\n",
    "        'total_size_gb': total_size_gb,\n",
    "        'batch_size': batch_size,\n",
    "        'n_batches': n_batches\n",
    "    }\n",
    "\n",
    "# Run production workflow analysis\n",
    "if not files_df.empty:\n",
    "    workflow_info = create_production_workflow(\n",
    "        data_dir, selected_date, selected_init_hour, max_files=20  # Limit for demo\n",
    "    )\n",
    "else:\n",
    "    workflow_info = None\n",
    "    print(\"‚ùå No files available for production workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6530b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and next steps\n",
    "print(\"üìã DASK MULTI-GRIB ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(f\"\\n‚úÖ ACCOMPLISHED:\")\n",
    "print(f\"  ‚Ä¢ Parallel GRIB file loading with Dask\")\n",
    "print(f\"  ‚Ä¢ Memory-efficient processing with chunked arrays\")\n",
    "print(f\"  ‚Ä¢ Multi-dimensional dataset creation (time √ó variables √ó space)\")\n",
    "print(f\"  ‚Ä¢ Lazy computation for large-scale analysis\")\n",
    "print(f\"  ‚Ä¢ Scalable visualization workflows\")\n",
    "print(f\"  ‚Ä¢ Production-ready batch processing framework\")\n",
    "\n",
    "if combined_ds is not None:\n",
    "    print(f\"\\nüìä CURRENT DATASET:\")\n",
    "    print(f\"  ‚Ä¢ Shape: {combined_ds.fire_weather_prob.shape}\")\n",
    "    print(f\"  ‚Ä¢ Forecast hours: {len(combined_ds.forecast_hour)}\")\n",
    "    print(f\"  ‚Ä¢ Variables: {len(combined_ds.variable)}\")\n",
    "    print(f\"  ‚Ä¢ Spatial resolution: {len(combined_ds.latitude)} √ó {len(combined_ds.longitude)}\")\n",
    "    print(f\"  ‚Ä¢ Memory footprint: ~{combined_ds.fire_weather_prob.nbytes/1024**3:.1f} GB\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS FOR FULL IMPLEMENTATION:\")\n",
    "print(f\"  1. Scale to all forecast hours (currently tested with {len(valid_datasets) if 'valid_datasets' in locals() else 0})\")\n",
    "print(f\"  2. Implement batch processing for memory management\")\n",
    "print(f\"  3. Add error handling and retry mechanisms\")\n",
    "print(f\"  4. Create automated verification pipelines\")\n",
    "print(f\"  5. Optimize chunk sizes based on analysis patterns\")\n",
    "print(f\"  6. Add data persistence/caching for repeated analysis\")\n",
    "\n",
    "print(f\"\\nüîó INTEGRATION WITH EXISTING WORKFLOW:\")\n",
    "print(f\"  ‚Ä¢ All projection information preserved from proof-of-concept\")\n",
    "print(f\"  ‚Ä¢ Compatible with cartopy/matplotlib visualization\")\n",
    "print(f\"  ‚Ä¢ CF-1.8 compliant metadata maintained\")\n",
    "print(f\"  ‚Ä¢ Can export to NetCDF, Zarr, or other formats\")\n",
    "\n",
    "# Resource monitoring\n",
    "print(f\"\\nüíª CURRENT RESOURCE USAGE:\")\n",
    "try:\n",
    "    cluster_info = client.scheduler_info()\n",
    "    print(f\"  ‚Ä¢ Active workers: {len(cluster_info['workers'])}\")\n",
    "    print(f\"  ‚Ä¢ Total cores: {sum(w['nthreads'] for w in cluster_info['workers'].values())}\")\n",
    "    print(f\"  ‚Ä¢ Total memory: {sum(w['memory_limit'] for w in cluster_info['workers'].values()) / 1024**3:.1f} GB\")\n",
    "except:\n",
    "    print(f\"  ‚Ä¢ Cluster info not available\")\n",
    "\n",
    "print(f\"\\nüéØ This notebook demonstrates successful scaling from single-file\")\n",
    "print(f\"   proof-of-concept to multi-file Dask-powered analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7746239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "print(\"üßπ Cleaning up Dask resources...\")\n",
    "\n",
    "# Close client and cluster\n",
    "try:\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    print(\"‚úÖ Dask client and cluster closed successfully\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Error closing Dask resources (may already be closed)\")\n",
    "\n",
    "print(\"\\nüéâ Multi-file JFWPRB analysis with Dask complete!\")\n",
    "print(\"üìù Ready for production deployment with full forecast hour coverage.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
